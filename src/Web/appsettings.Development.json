{
  "Serilog": {
    "MinimumLevel": {
      "Default": "Verbose",
      "Override": {
        "Microsoft": "Warning",
        "Microsoft.Hosting.Lifetime": "Information",
        "Microsoft.EntityFrameworkCore": "Information"
      }
    },
    "WriteTo": [
      {
        "Name": "Console",
        "Args": {
          "OutputTemplate": "{Timestamp:yyyy-MM-dd HH:mm:ss.fff zzz} [{Level:u3}] {SourceContext}[{ThreadId}] {Message:lj}{NewLine}{Exception}"
        }
      }
    ],
    "Enrich": [
      "FromLogContext",
      "WithThreadId",
      "WithExceptionDetails"
    ]
  },
  "LLM": "openai",
  "KernelMemory": {
    "Services": {
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": 5672,
        "Username": "user",
        "Password": "password"
      },
      "Postgres": {
        "ConnectionString": null,
        "TableNamePrefix": "memory_",
        "Columns": {
          "id": "_pk",
          "embedding": "embedding",
          "tags": "labels",
          "content": "chunk",
          "payload": "extras"
        },
        "CreateTableSql": [
          "BEGIN;                                                                      ",
          "SELECT pg_advisory_xact_lock(%%lock_id%%);                                  ",
          "CREATE TABLE IF NOT EXISTS %%table_name%% (                                 ",
          "  _pk         TEXT NOT NULL PRIMARY KEY,                                    ",
          "  embedding   vector(%%vector_size%%),                                      ",
          "  labels      TEXT[] DEFAULT '{}'::TEXT[] NOT NULL,                         ",
          "  chunk       TEXT DEFAULT '' NOT NULL,                                     ",
          "  extras      JSONB DEFAULT '{}'::JSONB NOT NULL,                           ",
          "  _update     TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP            ",
          ");                                                                          ",
          "CREATE INDEX ON %%table_name%% USING GIN(labels);                           ",
          "CREATE INDEX ON %%table_name%% USING ivfflat (embedding vector_cosine_ops); ",
          "COMMIT;                                                                     "
        ]
      },
      "Qdrant": {
        "Endpoint": "http://127.0.0.1:6333",
        "ApiKey": null
      },
      "OpenAI": {
        "TextEmbeddingGeneration": {
          "APIKey": null,
          "EmbeddingModel": "text-embedding-3-large",
          "EmbeddingModelMaxTokenTotal": 8191
        },
        "TextGeneration": {
          "APIKey": null,
          "TextModel": "gpt-4-turbo",
          "TextModelMaxTokenTotal": 128000,
          "TextGenerationType": "Auto"
        }
      },
      "LLamaSharp": {
        "TextGeneration": {
          // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
          "ModelPath": null,
          // Max number of tokens supported by the model
          "ContextSize": 8192,
          // Optional parameters
          "GpuLayerCount": 5,
          // ID девайса
          "MainGpu": 0,
          "Seed": 1337,
          // None - Single GPU
          // Layer - Split layers and KV across GPUs
          // Row - split rows across GPUs
          "SplitMode": "None"
        },
        "TextEmbeddingGeneration": {
          // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
          "ModelPath": null,
          // Max number of tokens supported by the model
          "ContextSize": 8192,
          // Optional parameters
          "GpuLayerCount": 5,
          // ID девайса
          "MainGpu": 0,
          "Seed": 1337,
          // None - Single GPU
          // Layer - Split layers and KV across GPUs
          // Row - split rows across GPUs
          "SplitMode": "None"
        }
      },
      "LMStudio": {
        "TextGeneration": {
          "APIKey": "lm-studio",
          "Endpoint": "http://localhost:1234/v1/",
          "TextModel": "local-model",
          "TextModelMaxTokenTotal": 8192
        }
      }
    },
    "NERSearchClient": {
      // Сколько чанков использовать, если документ будет слишком большой, сервис будет очень долго отрабатывать
      "MaxMatchesCount": 100,
      // Максимальное кол-во токенов на ответ
      "AnswerTokens": -1
    },
    "DataIngestion": {
      "TextPartitioning": {
        // The maximum number of tokens per paragraph.
        // When partitioning a document, each partition usually contains one paragraph.
        // Зависит от размерности эмбеддера
        "MaxTokensPerParagraph": 1000,
        // The maximum number of tokens per line, aka per sentence.
        // When partitioning a block of text, the text will be split into sentences, that are then grouped into paragraphs.
        // Note that this applies to any text format, including tables, code, chats, log files, etc.
        "MaxTokensPerLine": 300,
        // The number of overlapping tokens between paragraphs.
        "OverlappingTokens": 100
      }
    }
  }
}
